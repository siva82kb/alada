\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{
    a4paper,
    total={170mm,257mm},
    left=25mm,
    right=25mm,
    top=25mm,
    bottom=25mm,
}
\usepackage{multicol}
\usepackage[font=small,labelfont=bf]{caption}
\setlength{\columnsep}{0.25cm}
\usepackage[inline]{enumitem}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{mathtools} 
\setlength{\parindent}{0em}
\setlength{\parsep}{0em}
\usepackage{tikz}
\setlength{\parskip}{0em}
\usetikzlibrary{decorations.pathmorphing,patterns}
\usepackage[american,cuteinductors]{circuitikz}
\usetikzlibrary{shapes,arrows,circuits,calc,babel}
% Definition of blocks:
\tikzset{%
  block/.style    = {draw, thick, rectangle, minimum height = 3em,
    minimum width = 3em},
  sum/.style      = {draw, circle, node distance = 2cm}, % Adder
  input/.style    = {coordinate}, % Input
  output/.style   = {coordinate} % Output
}
% Defining string as labels of certain blocks.
\newcommand{\suma}{\Large$+$}
\newcommand{\inte}{$\displaystyle \int$}
\newcommand{\derv}{\huge$\frac{d}{dt}$}

\def\mf{\ensuremath\mathbf}
\def\mb{\ensuremath\mathbb}
\def\mc{\ensuremath\mathcal}
\def\lp{\ensuremath\left(}
\def\rp{\ensuremath\right)}
\def\lv{\ensuremath\left\lvert}
\def\rv{\ensuremath\right\rvert}
\def\lV{\ensuremath\left\lVert}
\def\rV{\ensuremath\right\rVert}
\def\lc{\ensuremath\left\{}
\def\rc{\ensuremath\right\}}
\def\ls{\ensuremath\left[}
\def\rs{\ensuremath\right]}
\def\bmx{\ensuremath\begin{bmatrix*}[r]}
\def\emx{\ensuremath\end{bmatrix*}}
\def\bmxc{\ensuremath\begin{bmatrix*}[c]}
\def\emxc{\ensuremath\end{bmatrix*}}
% \def\t{\lp t\rp}
% \def\k{\ls k\rs}

\newcommand{\demoex}[2]{\onslide<#1->\begin{color}{black!60} #2 \end{color}}
\newcommand{\demoexc}[3]{\onslide<#1->\begin{color}{#2} #3 \end{color}}
\newcommand{\anim}[3]{\onslide<#1->{\begin{color}{#2!60} #3 \end{color}}}
\newcommand{\ct}[1]{\lp #1\rp}
\newcommand{\dt}[1]{\ls #1\rs}

% \renewcommand{\familydefault}{\sfdefault}

\begin{document}
\begin{center}
\begin{Large}
\textbf{Applied Linear Algebra in Data Analaysis}\\
\vspace{0.1cm}
\textbf{Orthogonality \& Matrix Inverses Assignment}
\end{Large}
\end{center}
\hrule
\vspace{0.2cm}

\begin{enumerate}
  \item Consider an orthonormal set of vectors,
  \[ V = \left\{ \mf{v}_1, \mf{v}_2, \ldots \mf{v}_r\right\} \quad \mf{v}_i \in \mb{R}^n \quad \forall i \in \left\{1, 2, \ldots r\right\}\]
  If there is a vector $\mf{w} \in \mb{R}^n$ such that $\mf{v}_i^T\mf{w} = 0\,\,\,\forall i \in \left\{1, 2, \ldots r\right\}$. Prove that $\mf{w} \notin span\left(V\right)$.
  
  \item Consider the following set of vectors in $\mb{R}^4$.
  \[ V = \left\{
  \bmx
  1\\-2\\0\\3
  \emx,
  \bmx
  1\\1\\1\\1
  \emx,
  \bmx
  2\\-1\\1\\4
  \emx
  \right\} \]
  Find the set of all vectors that are orthogonal to $V$?

  \item For a matrix $\mf{A} \in \mb{R}^{m \times n}$, prove that $C\lp\mf{A}\rp \perp N\lp\mf{A}^T\rp$ and $C\lp\mf{A}^T\rp \perp N\lp\mf{A}\rp$.

  \item If the columns of a matrix $\mf{A} \in \mb{R}^{n \times n}$ are orthonormal, prove that $\mf{A}^{-1} = \mf{A}^T$. What is $\mf{A}^T\mf{A}$ when $\mf{A}$ is rectangular $\left(\mf{A} \in \mb{R}^{m \times n}\right)$ with orthonormal columns?

  \item What will happen when the Gram-Schmidt procedure is applied to: (a) orthonormal set of vectors; and (b) orthogonal set of vectors? If the set of vectors are columns of a matrix $\mf{A}$, then what are the corresponding $\mf{Q}$ and $\mf{R}$ matrices for the orthonormal and orthogonal cases?

  \item Prove that the rank of an orthogonal projection matrix $\mf{P}_{S} = \mf{UU}^T$ onto a subspace $\mc{S}$ is equal to the $\text{dim } \mc{S}$, where the columns of $\mf{U}$ form an orthonormal basis of $\mc{S}$.

  \item If the columns of $\mf{A} \in \mb{R}^{m \times n}$ represent a basis for the subspace $\mc{S} \subset \mb{R}^m$. Find the orthogonal projection matrix $\mf{P}_\mc{S}$ onto the subspace $\mc{S}$. Hint: Gram-Schmidt orthogonalization.

  \item Consider two orthogornal matrices $\mf{Q}_1$ and $\mf{Q}_2$. Is the $\mf{Q}_2^T\mf{Q}_1$ an orthogonal matrix? If yes, prove that it is so, else provide a counter-example showing $\mf{Q}_2^T\mf{Q}_1$ is not orthogonal.

  \item Let $\mf{P}_\mc{S}$ represent an orthogonal projection matrix onto to the subspace $\mc{S} \subset \mb{R}^n$. What can you say about the rank of the matrix $\mf{P}_\mc{S}$? Explain how you can obtain an orthonormal basis for $\mc{S}$ from $\mf{P}_\mc{S}$.

  \item Consider a 1 dimensional subspace spanned by the vector $\mf{u} \in \mb{R}^n$. What kind of a geometric operation does the matrix $\mf{I} - 2\frac{\mf{u}\mf{u}^T}{\mf{u}^T\mf{u}}$ represent?

  \item Prove that when a triangular matrix is orthogonal, it is diagonal.

  \item If an orthogonal matrix $\mf{Q} \in \mb{R}^{n \times n}$ is to be partitioned such that, $\mf{Q} = \bmx \mf{Q}_1 & \mf{Q}_2\emx$, then prove that $C\lp\mf{Q}_1\rp \perp C\lp\mf{Q}_2\rp$.

  \item Find an orthonormal basis for the subspace spanned by $\lc \bmx1\\-1\\2\emx, \bmx-1\\-1\\-1\emx, \bmx1\\-3\\3\emx \rc$.

\end{enumerate}

\end{document}