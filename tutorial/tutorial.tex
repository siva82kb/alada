%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Arsclassica Article
% LaTeX Template
% Version 1.1 (1/8/17)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Lorenzo Pantieri (http://www.lorenzopantieri.net) with extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
10pt, % Main document font size
a4paper, % Paper type, use 'letterpaper' for US Letter paper
oneside, % One page layout (no page indentation)
%twoside, % Two page layout (page indentation for binding and different headers)
headinclude,footinclude, % Extra spacing for the header and footer
BCOR5mm, % Binding correction
]{scrartcl}

\input{structure.tex} % Include the structure.tex file which specified the document structure and layout

\hyphenation{Fortran hy-phen-ation} % Specify custom hyphenation points in words with dashes where you would like hyphenation to occur, or alternatively, don't put any dashes in a word to stop hyphenation altogether

\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{xcolor}

\def\mf{\ensuremath\mathbf}
\def\mb{\ensuremath\mathbb}
\def\lp{\ensuremath\left(}
\def\rp{\ensuremath\right)}
\def\lv{\ensuremath\left\lvert}
\def\rv{\ensuremath\right\rvert}
\def\lV{\ensuremath\left\lVert}
\def\rV{\ensuremath\right\rVert}
\def\lc{\ensuremath\left\{}
\def\rc{\ensuremath\right\}}
\def\ls{\ensuremath\left[}
\def\rs{\ensuremath\right]}
\def\bmx{\ensuremath\begin{bmatrix*}[r]}
\def\emx{\ensuremath\end{bmatrix*}}
\def\bmxc{\ensuremath\begin{bmatrix*}[c]}


%----------------------------------------------------------------------------------------
%	TITLE AND AUTHOR(S)
%----------------------------------------------------------------------------------------

\title{{Applied Linear Algebra in Data Analysis}} % The article title

\subtitle{{Tutorial}} % Uncomment to display a subtitle

\author{{Sivakumar Balasubramanian}} % The article author(s) - author affiliations need to be specified in the AUTHOR AFFILIATIONS block

\date{} % An optional date to appear under the author(s)

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	HEADERS
%----------------------------------------------------------------------------------------

\renewcommand{\sectionmark}[1]{\markright{\spacedlowsmallcaps{#1}}} % The header for all pages (oneside) or for even pages (twoside)
%\renewcommand{\subsectionmark}[1]{\markright{\thesubsection~#1}} % Uncomment when using the twoside option - this modifies the header on odd pages
\lehead{\mbox{\llap{\small\thepage\kern1em\color{halfgray} \vline}\color{halfgray}\hspace{0.5em}\rightmark\hfil}} % The header style

\pagestyle{scrheadings} % Enable the headers specified in this block

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LISTS OF FIGURES AND TABLES
%----------------------------------------------------------------------------------------

\maketitle % Print the title/author/date block

\setcounter{tocdepth}{2} % Set the depth of the table of contents to show sections and subsections only

\tableofcontents % Print the table of contents

%\listoffigures % Print the list of figures

%\listoftables % Print the list of tables

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

%\section*{Abstract} % This section will not appear in the table of contents due to the star (\section*)
%
%\lipsum[1] % Dummy text
%
%%----------------------------------------------------------------------------------------
%%	AUTHOR AFFILIATIONS
%%----------------------------------------------------------------------------------------
%
%\let\thefootnote\relax\footnotetext{* \textit{Department of Biology, University of Examples, London, United Kingdom}}
%
%\let\thefootnote\relax\footnotetext{\textsuperscript{1} \textit{Department of Chemistry, University of Examples, London, United Kingdom}}

%----------------------------------------------------------------------------------------

\newpage % Start the article content on the second page, remove this if you have a longer abstract that goes onto the second page

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

% -----
% CONCEPTS IN VECTOR SPACES
% -----
\section{Concepts in Vector Spaces}
\hrule
\vspace{0.5cm}

\begin{enumerate}

\item Which of the following sets forms a vector space?
\begin{enumerate}
	\item $\left\{ \mf{x} \, \vert \, x_1, x_2 \in \mathbb{R} \text{ and } a_1x_1 + a_2 x_2 = 0 \right\}$, where $a_1, a_2 \in \mathbb{R}$ are fixed constants.
	\item $\left\{ \mf{x} \, \vert \, \mathbf{x} \in \mathbb{R}^n \text{ and } \mathbf{a}^\top \mathbf{x} = b \right\}$, where $\mathbf{a} \in \mathbb{R}^n$ and $b \in \mathbb{R}$ are fixed constants.
	\item $\left\{ \mf{x} \, \vert \, \mathbf{x} \in \mathbb{R}^n \text{ and } \mathbf{x}^\top \mathbf{x} = 1 \right\}$.
	\item $\left\{ \left(x[0], x[1], x[2], \ldots x[N-1]\right) \, \vert \, x[i] \in \mathbb{R}, 0 \leq i < N \right\}$.
	
	\textcolor{blue}{(\small{The set of all real-valued time-domain signals of length $N$. $x[i]$ is the value of the signal at time instant $i$.})}
\end{enumerate}

\item Consider the vector space of polynomials of order $n$ or less.
\[ \mathcal{P} = \left\{ \sum_{k=0}^n a_k x^k \, \bigg\vert \, a_k \in \mathbb{R} \right\}, \,\, \text{where, } x \in \left[0, 1\right] \]
Show that polynomails of order strictly lower than $n$ form subspaces of $\mathcal{P}$.

\item Is the following function a valid norm of the vector space $\mathcal{P}$?
\[ \Vert \mathbf{p}\pp{x} \Vert = \sqrt{\sum_{k=0}^{n} a_k^2 }, \,\, \mathbf{p} = \sum_{k=0}^n a_kx^k \in \mathcal{P} \]

\item Consider the following function, which is often called the \textit{zero-norm} of a vector $\mathbf{x} \in \mathbb{R}^n$.
\[ \Vert \mathbf{x} \Vert_0 = \sum_{i=1}^n \mathbb{I}\pp{x_i \neq 0}, \,\, \text{where, } \mathbb{I}\pp{A} = \begin{cases} 1 & A \text{ is true.} \\ 0 & A \text{ is false.}  \end{cases}  \] 
Is the \textit{zero-norm}, which is often used for quantifying the \textit{sparsity} of a vector, a proper norm?

\item Is the following set of vectors linear independent?
\[ \left\{ \bmx 1 \\ 0 \\ 0 \\ 0 \emx, \bmx 1 \\ 2 \\ 0 \\ 0 \emx , \bmx 1 \\ 1 \\ -1 \\ 1 \emx , \bmx 1 \\ 2 \\ 0 \\ -1 \emx  \right\} \]

What is the span of this set? Does this set form the basis for its span? Does it form an orthonormal basis?

\item Consider the following function,
\[ f\pp{\mf{x}} = \sum_{i=1}^n w_i \vert x_i \vert, \,\,\, \mf{x} \in \mb{R}^n, w_i > 0 \]
Is $f$ a norm? If not, what properties does it lack?

\item Find the norm of the following vectors using the the 1-norm, 2-norm and the $\infty$-norm.
\begin{enumerate}
	\item $\mf{x} = \bmx 1 & 2 & 3 \emx^\top$
	\item $\mf{x} = \bmx 1 & -1 & 0 \emx^\top$
	\item $\mf{e}_i$, where $1 \leq i \leq n$
	\item $\mf{0} \in \mb{R}^n$
	\item $\mf{1} \in \mb{R}^n$
\end{enumerate}

\item For any given $\mf{x} \in \mb{R}^n$, show that,
$$ \Vert \mf{x} \Vert_1 \geq \Vert \mf{x} \Vert_2 \geq \Vert \mf{x} \Vert_3 \cdots \geq \Vert \mf{x} \Vert_\infty $$

\item Consider the linear function $f : \mb{R}^3 \mapsto \mb{R}$. We know the output of the function for the following inputs,
\[ f\pp{\bmx 1 & 1  & 1\emx^\top} = -2, \quad f\pp{\bmx -1 & 2  & -1\emx^\top} = 1, \quad f\pp{\bmx -1 & 1  & 2\emx^\top} = 0 \]
Find an input input $\mf{x} \in \mb{R}^3$ such that $f\pp{\mf{x}} = 0$.

\item Find the representation of $\mf{x} = \bmx 2 & 1\emx^\top$ in the following bases.
\begin{enumerate}
	\item $\lc \bmx 1 \\ 0\emx, \bmx 0 \\ 1 \emx \rc$
	\item $\frac{1}{\sqrt{2}}\lc \bmx 1 \\ 1\emx, \frac{1}{\sqrt{2}}\bmx 1 \\ -1 \emx \rc$
	\item $\lc \bmx 2 \\ 1\emx, \bmx 1 \\ 1 \emx \rc$
\end{enumerate}

\item Consider the function $f_i : \mb{R}^n \mapsto \mb{R}$ that selects the $i^{th}$ element of a given vector $\mf{x} \in \mb{R}^n$.
\[ f\pp{\mf{x}} = x_i, \text{where} \quad \mf{x} = \bmx x_1 & x_2 \cdots & x_n \emx^\top \]
Is this function linear? If so, what is the vector $\mf{w}$ asssociated with this function, such that $f\pp{\mf{x}} = \mf{w}^\top\mf{x}$?

\item Given a set of real numbers $x_1, x_2, \ldots x_n \in \mb{R}$ which are used to the $n$-vector $\mf{x}$. Can you express the mean $\overline{x}$ and variance $\sigma_x^2$ of this set of data using the the standard inner product in $\mb{R}^n$? Note the following,
\[ \overline{x} = \frac{1}{n}\sum_{i=1}^n x_i \quad \quad \quad \sigma_x^2 = \frac{1}{n-1}\sum_{i=1}^n \pp{x_i - \overline{x}}^2 \]
\end{enumerate}

\newpage

% -----
% MATRICES
% -----
\section{Matrices}
\hrule
\vspace{0.5cm}

\begin{enumerate}
	\item Conisder the following matrices:
	\[ \mf{A} = \bmx 1 & 1 & -1 & 0 \\
	0 & 2 & -2 & 1 \\
	-3 & 1 & 1 & 3
	\emx \quad \text{and} \quad \mf{B} = \bmx 2 & 1 & 1 \\
	1 & -1 & 1 \\
	3 & 2 & 1 \\
	1 & 2 & 1
	\emx \]
	\begin{enumerate}
		\item Find the product of the two matrices $\mf{C} = \mf{A}\mf{B}$ using the four views of matrix muliplication.

		\item If we change $b_{23} = 0$. Can you compute the new matrix $\mf{C}$ without performing the entire matrix muliplication again?
	
		\item If we increase the value of the elements of the 3$^{rd}$ column of $\mf{A}$ by $1$, how can we compute the new $\mf{C}$ without performing the entire matrix multiplication again?
	
		\item If we insert a new row $\mf{1}^\top$ in $\mf{A}$ after the 2$^{nd}$ row of $\mf{A}$, how can we compute the new $\mf{C}$ without performing the entire matrix multiplication again?
	\end{enumerate}
	
	\item Consider a matrix $\mf{A} \in \mb{R}^{10^6 \times 5}$, and we are interested in computing the product $\mf{A}^\top\mf{A}\mf{A}^\top$. Should you compute the product as $\lp \mf{A}^\top\mf{A} \rp\mf{A}^\top$ or $\mf{A}^\top\lp \mf{A}\mf{A}^\top \rp$? Why?
	
	\item Consider an orthogonal, square matrix $\mf{A} \in \mb{R}^{n \times n}$. We generate a new matrix $\mf{C} = \mf{A}\mf{B}$. What can we say about the following questions about this product?
	\begin{enumerate}
		\item How are the columns of $\mf{C}$ related to the columns of $\mf{A}$
		\item How is the 2-norm of the $i^{th}$ column of $\mf{C}$ related that of the columns of $\mf{B}$? 
	\end{enumerate}

	\item Show that the matrix product $\mf{A}\mf{B}\mf{C}$ can be written as a weighted sum  of the outer products of the columns of $\mf{A} \in \mb{R}^{n \times p}$ and rows of $\mf{C} \in \mb{R}^{q \times n}$, with the weights coming from the matrix $\mf{B} \in \mb{R}^{p \times q}$.
	\[ \mf{A}\mf{B}\mf{C} = \sum_{i=1}^p \sum_{j=1}^q b_{ij} \mf{a}_i \tilde{\mf{c}}_j^\top \]

	\item Prove the following for the matrices $\mf{A}_1, \mf{A}_2, \mf{A}_3, \ldots \mf{A}_n$.
	\[ \lp \mf{A}_1, \mf{A}_2 \mf{A}_3 \ldots \mf{A}_n \rp^\top =  \mf{A}_n^\top \mf{A}_{n-1}^\top \ldots \mf{A}_2^\top \mf{A}_1^\top \]

	% \item \textbf{Nilpotent matrices}. Show that a strictly triangular matrix $\mf{A} \in \mb{R}^{n \times n}$, $\mf{A}^n = \mf{0}$.

	\item \textbf{Matrix Inversion Lemma}. Consider an invertible matrix $\mf{A}$. The matrix $\mf{A} + \mf{u}\mf{v}^\top$ is invertible if and only if the two vectors $\mf{u}, \mf{v} \neq \mf{0}$, and $\mf{v}^\top\mf{A}^{-1}\mf{u} \neq -1$. Then, the inverse is given by,
	\[ \lp \mf{A} + \mf{u}\mf{v}^\top\rp^{-1} = \mf{A}^{-1} - \frac{\mf{A}^{-1}\mf{u}\mf{v}^\top\mf{A}^{-1}}{1 + \mf{v}^{\top}\mf{A}^{-1}\mf{u}} \]

	\item Prove that $tr\lp \mf{A}\mf{B} \rp = tr\lp \mf{B}\mf{A} \rp$, where $\mf{A} \in \mb{R}^{n \times d}$ and $\mf{B} \in \mb{R}^{d \times n}$.
	
	\item Show that the diagonal elements of a square matrix $\mf{A}$, such that $\mf{A}^\top = -\mf{A}$ are zero. These are \textit{skew-symmetric} matrices.
	
	\item Show that $\mf{x}^\top\mf{A}\mf{x} = 0$ if $\mf{A} \in \mb{R}^{n \times n}$ is a skew-symmetric matrix.
\end{enumerate}

\newpage
\noindent \textbf{Additional problems}
\begin{enumerate}
	\item Consider the following $5 \times 4$ matrix:
	\[ \mf{A} = \bmx  1 & -1 &  0 &  2 &  2 \\
	 1 & -2 &  1 &  3 &  9 \\
	-2 &  0 &  2 & -1 & -2 \\
	 3 &  1 &  1 & -5 &  0
	\emx \]
	Compute the following:
	\begin{enumerate}
		\item $\mf{a}_3$
		\item $\mf{a}_1^\top$
		\item $\tilde{\mf{a}}_2^\top$
		\item $\tilde{\mf{a}}_4$
		\item $\mf{a}_1\mf{a}_2^\top$
		\item $\tilde{\mf{a}}_3\mf{a}_2^\top$
		\item $\tilde{\mf{a}}_1\tilde{\mf{a}}_2^\top$
		\item $\tilde{\mf{a}}_1^\top\tilde{\mf{a}}_2$
		\item $\tilde{\mf{a}}_1\tilde{\mf{a}}_1^\top + \tilde{\mf{a}}_2\tilde{\mf{a}}_2^\top$
		\item $\mf{a}_3^\top\mf{a}_1 + \mf{a}_2^\top\mf{a}_4$
	\end{enumerate}

	\item Which of the following statements true about a matrix $\mf{A} \in \mb{R}^{n \times m}$?
	\begin{enumerate}
		\item $\sum_{i=1}^m \Vert \mf{a}_i \Vert_2^2 = \sum_{i=1}^n \Vert \tilde{\mf{a}}_i \Vert_2^2$
		\item $\sum_{i=1}^m \Vert \mf{a}_i \Vert_2^2 = \text{tr}\pp{\mf{A}^\top\mf{A}}$
		\item If the matrix $\mf{A} = \mf{V} \bmx d_1 &  0 \\ 0 & d_2\emx \mf{V}^{-1}$, Then
		\[ \mf{A}^n = \mf{V} \bmx d_1^n & 0 \\ 0 & d_2^n \emx \mf{V}^{-1} \]
	\end{enumerate}
	\item Consider the matrix $\mf{P} = \bmx \mf{e}_1 & \mf{e}_3 & \mf{e}_2 \emx$. What does this P matrix do to a matrix $\mf{A} \in \mb{R}^{3 \times 3}$ in the following operations? Try to compute these without performing the matrix multiplication and by using you understaning of the row and column views of matrix multiplication.
	\begin{enumerate}
		\item $\mf{P}\mf{A}$
		\item $\mf{A}\mf{P}$
		\item $\mf{P}^2\mf{A}$
		\item $\mf{A}\mf{P}^2$
		\item $\mf{P}\mf{A}\mf{P}$
	\end{enumerate}
\end{enumerate}

\newpage

% -----
% SOLUTION TO LINEAR EQUATIONS
% -----
\section{Solution to Linear Equations}
\hrule
\vspace{0.5cm}

\begin{enumerate}	
	\item Consider a matrix $\mf{A} \in \mb{}R^{n \times m}$ with $n > m$. Consider a linearly independent set of vectors $\mf{x}_1, \mf{x}_2$. Is the set of $\mf{A}\mf{x}_1, \mf{A}\mf{x}_2$ linearly independent?
	
	\item Find the bases for the four fundamental subspaces of the following matrices 
	\begin{enumerate}
		\item $\mf{A} = \bmx 1 & 1 & 1 \\ 2 & 2 & 2 \\ -1 & -1 & -1 \emx$.
		\item $\mf{A} = \bmx 1 & 1 & 1\emx$.
		\item $\mf{A} = \bmx 1 & 1 \\ 2 & 1\emx$.
		\item $\mf{A} = \bmx 0 & 0 \\ 0 & 0\emx$.
		\item $\mf{A} = \bmx 1 & 0 \\ 0 & 1\emx$.
	\end{enumerate}
	
	\item Show that the $rank\pp{\mf{A}\mf{B}} = rank\pp{\mf{A}}$, when $\mf{B}$ is square and full rank.

	\item Let $\mf{A}$ is a full rank matrix. Show that the \textit{Gram matrix} of the column space, $\mf{A}^\top\mf{A}$ is invertible.
	
	\item Draw the four fundamental subspaces of the matrix $\mf{A} = \bmx 1 & 1 \\ 2 & 2 \emx$.
	
	\item Find the complete set of solutions for the following system of equations,
	\[ \mf{A}\mf{x} = \bmx 4 \\ 3 \\ 7 \emx \]
	where $\mf{A} = \bmx 1 & 1 & 2\\ 2 & 2 & -1\\ 3 & 3 & 1\emx$.
	
	\item Is the following set of equations solvable? If yes, then find the complete set of solutions.
	\[ \mf{A}\mf{x} = \bmx 1 \\ 1 \\ 1 \emx \]
	where $\mf{A} = \bmx 1 & 1 & 2\\ 2 & 2 & -1\\ 3 & 3 & 1\emx$.
\end{enumerate}

\newpage
% -----
% ORTHOGONALITY
% -----
\section{Orthogonality}
\hrule
\vspace{0.5cm}
\begin{enumerate}
	\item If $\mf{A}$ is an orthogonal matrix, show that $\mf{A}^{-1} = \mf{A}^\top$.
	
	\item If $\mf{P}_{\mathcal{S}}$ is the orthogonal projection matrix onto the subspace $\mathcal{S}$, then what is the corresponding orthogonal projection matrix onto $\mathcal{S}^{\perp}$ -- the orthogonal complement of $\mathcal{S}$?
	
	\item Let $\mf{x}, \mf{y} \in \mathbb{R}^n$. Let $\cc{\mf{u}_1, \mf{u}_2, \ldots \mf{u}_n}$ be an orthonormal basis for $\mathbb{R}^n$. Show that the following holds,
	\[ \mf{x}^\top\mf{y} = \sum_{i=1}^n \pp{\mf{x}^\top \mf{u_i}} \cdot \pp{\mf{u_i}^\top \mf{y}} \]
	
	\item Consider the following set of vectors, $\mathcal{S} = \left\{ \mf{a}_1, \mf{a}_2, \mf{a}_3, \ldots \mf{a}_n \right\}$, where $\mf{a}_i \in \mathbb{R}^n$. The set $\mc{S}$ is linearly independent. Find the orthogonal components of a vector $\mf{b} \in \mathbb{R}^n$ in the subspace spanned by the sets of vectors $\mathcal{S}_1 = \left\{ \mf{a}_i \right\}_{i=1}^m$ and $\mathcal{S}_1^\perp$.
	
	\item Consider the set of $n \times n$ orthogonal matrices,
	\[ \mc{Q} = \left\{ \mf{Q} \, \big \vert \, \mf{Q} \in \mb{R}^{n \times n}, \, \mf{Q}^\top\mf{Q} = \mf{Q}\mf{Q}^\top = \mf{I}_n  \right\} \] 
	Is this set a subspace of $\mb{R}^{n \times n}$? \\
	Show that the set is closed under matrx multiplication.
	
    \item Consider the linear map, $\mf{y} = \mf{Ax}$, such that $\mf{x}, \mf{y} \in \mb{R}^n$ and $\mf{A} \in \mb{R}^{n \times n}$. Let us assume that $\mf{A}$ is full rank. What conditions must $\mf{A}$ satisfy for the following statements to be true,
    \begin{enumerate}
        \item $\left\lVert \mf{y}\right\rVert_2 = \left\lVert \mf{x}\right\rVert_2$, for all $\mf{x}, \mf{y}$ such that $\mf{y} = \mf{Ax}$.
        \item $\mf{y}_1^T\mf{y}_2 = \mf{x}_1^T\mf{x}_2$, for all $\mf{x}_1, \mf{x}_2, \mf{y}_1, \mf{y}_2$ such that $\mf{y}_1 = \mf{A}\mf{x}_1$ and $\mf{y}_2 = \mf{A}\mf{x}_2$. 
    \end{enumerate}
    \vspace{-0.1cm}
    \begin{color}{black!60}\small{\textbf{Note}: A linear map $\mf{A}$ with the aforementioned properties preserves lengths and angle between vectors. Such maps are encountered in rigid body mechanics.}
    \end{color}

	\item Find the QR demcomposition of the following matrices.
	\begin{enumerate}
		\item $\mf{A} = \bmx 1 & 1 \\ 0 & 1 \\ 1 & 0 \emx$
		\item $\mf{A} = \bmx 1 & 1 & 1 \\ 1 & 1 & 0 \\ 1 & 0 & 1 \emx$
		\item $\mf{A} = \bmx 1 & 1 & 1 & 0 & -1 \\ 1 & 1 & 0 & 0 & -1\\ 1 & 0 & 1 & 0 & -1\emx$
		\item $\mf{A} = \bmx 1 & 1 & 1 \\ 1 & 1 & 0 \\ 1 & 0 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1 \emx$
		\item $\mf{A} = \bmx 3 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 5 \emx$
	\end{enumerate}
\end{enumerate}


\newpage
% -----
% MATRIX INVERSES
% -----
\section{Matrix Inverses}
\hrule
\vspace{0.5cm}

\begin{enumerate}
	\item Find a left inverse for the matrix $\mf{A} = \bmx 1 & 1 \\ -1 & 1 \\ 0 & 1\emx$. Find the set of all possible left inverses. 
	
	\item Show that if the product of two square $d \times d$ matrices $\mf{A}$ and $\mf{B}$ is the identity matrix $\mf{I}$, then $\mf{B}\mf{A} = \mf{I}$.
	
	\item Consider an upper triangular matrix $\mf{R} \in \mb{R}^{n \times n}$. We are interested in solving the following set of $n$ linear equations,
	\[ \mf{R} \mf{x} = \mf{e}_i \]
	$\mf{x} = \bmxc x_1 & x_2 & x_3 & \ldots & x_n \emx^\top$ is the solution to the above equation. Show that $x_{i+1} = x_{i+2} = \ldots = x_n = 0$.

	Show that the solution to this equation is equal to the $i^{th}$ column of the inverse of $\mf{R}$.

	\item Find the pseudo-inverse of the matrix $\mf{A} = \bmx 1 & 1 \\ 1 & -1 \\ 1 & 1 \emx$. Show that the matrix $\mf{A}\mf{A}^\dagger$ is the orthogonal projection matrix onto the column space of $\mf{A}$.
\end{enumerate}

\end{document}