\documentclass[aspectratio=169]{beamer}

\setbeamersize{text margin left=5mm, text margin right=5mm}

\defbeamertemplate{headline}{my header}{%
\vskip1pt%
\makebox[0pt][l]{\,\insertshortauthor}%
\hspace*{\fill}\insertshorttitle/\insertshortsubtitle\hspace*{\fill}%
\llap{\insertpagenumber/\insertpresentationendpage\,}
}
\setbeamertemplate{headline}[my header]

\let\olditem\item
\renewcommand{\item}{\setlength{\itemsep}{\fill}\olditem}

\usepackage{bm}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{soul}
\usepackage{tkz-euclide}
\usetikzlibrary{calc}
\usepackage[]{algorithm2e}
\usepackage{changepage}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{tcolorbox}
\usepackage{tikz}
\usepackage{tikz-3dplot}
\usepackage{tkz-euclide}
\usepackage{pgfplots}
\pgfplotsset{width=5cm,compat=1.3}
\usepackage{circuitikz}
\usepackage{mleftright}
\usetikzlibrary{arrows.meta, decorations.pathreplacing, positioning, shapes.geometric}
\usetikzlibrary{arrows}

\usepackage{pgfplots}
\usetikzlibrary{plotmarks}
\pgfplotsset{width=7cm,compat=1.8}
\pgfplotsset{compat=1.17}

\usetikzlibrary{positioning}
% \usepackage[math]{cellspace}
% \cellspacetoplimit 4pt
% \cellspacebottomlimit 4pt
%\usetikzlibrary{arrows.meta}
% sqare of half axes
\newcommand{\asa}{3}
\newcommand{\bsa}{1}
\newcommand{\csa}{0.25}
% view angle
\tdplotsetmaincoords{70}{135}


%% Fonts
\usefonttheme{professionalfonts}
\usefonttheme{serif}

\DeclareCaptionLabelFormat{blank}{}
\captionsetup[figure]{labelformat=blank}

%% Math definitions
\def\mf{\ensuremath\mathbf}
\def\mb{\ensuremath\mathbb}
\def\mc{\ensuremath\mathcal}
\def\lp{\ensuremath\left(}
\def\rp{\ensuremath\right)}
\def\lv{\ensuremath\left\lvert}
\def\rv{\ensuremath\right\rvert}
\def\lV{\ensuremath\left\lVert}
\def\rV{\ensuremath\right\rVert}
\def\lc{\ensuremath\left\{}
\def\rc{\ensuremath\right\}}
\def\ls{\ensuremath\left[}
\def\rs{\ensuremath\right]}
\def\bmx{\ensuremath\begin{bmatrix*}[r]}
\def\emx{\ensuremath\end{bmatrix*}}
\def\bmxc{\ensuremath\begin{bmatrix*}[c]}
\def\R{\ensuremath\mb{R}}
\def\t{\lp t\rp}
\def\k{\ls k\rs}

\newcommand{\demoex}[2]{\onslide<#1->\begin{color}{black!60} #2 \end{color}}
\newcommand{\demoexc}[3]{\onslide<#1->\begin{color}{#2} #3 \end{color}}
\newcommand{\anim}[3]{\onslide<#1->{\begin{color}{#2!60} #3 \end{color}}}
\newcommand{\ct}[1]{\lp #1\rp}
\newcommand{\dt}[1]{\ls #1\rs}
\newcommand{\cols}[2]{\begin{columns}[#1] #2 \end{columns}}
\newcommand{\col}[2]{\begin{column}{#1} #2 \end{column}}

%% Mycolors
\definecolor{myred}{RGB}{192,0,0}
\definecolor{mygray}{RGB}{100,100,100}

%% Custom beamer color
\setbeamercolor{title}{fg=myred}
\setbeamercolor{subtitle}{fg=myred}
\setbeamerfont{title}{series=\bfseries}
% \setbeamercolor{frametitle}{bg=myred, fg=white}
\setbeamercolor{frametitle}{bg=mygray!10!, fg=myred}
\setbeamerfont{frametitle}{series=\bfseries}
\setbeamercolor{item}{fg=mygray}
\setbeamercolor{title in head/foot}{fg=myred}

% Move header to footer
\setbeamertemplate{headline}{}
\setbeamertemplate{footline}{
  \begin{beamercolorbox}[wd=\paperwidth,ht=2.25ex,dp=1ex,center]{footline}
    \inserttitle\hfill\insertauthor\hfill\insertdate\hfill\insertframenumber{}
  \end{beamercolorbox}
}

\pgfplotsset{
colormap={whitered}{color(0cm)=(white); color(1cm)=(orange!75!red)}
}


\title{Applied Linear Algebra in Data Analysis}

% A subtitle is optional and this may be deleted
\subtitle{Introduction to Optimization}

\author{Sivakumar Balasubramanian}
% - Give the names in the same order as the appear in the paper.
% - Use the \inst{?} command only if the authors have different
%   affiliation.

\institute[Christian Medical College] % (optional, but mostly needed)
{
  \inst{}%
  Department of Bioengineering\\
  Christian Medical College, Bagayam\\
  Vellore 632002
}
% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.

\date{}
% - Either use conference name or its abbreviation.
% - Not really informative to the audience, more for people (including
%   yourself) who are reading the slides online

\subject{Lecture notes on ALADA}
% This is only inserted into the PDF information catalog. Can be left
% out. 

% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% \pgfdeclareimage[height=0.5cm]{university-logo}{university-logo-filename}
% \logo{\pgfuseimage{university-logo}}

% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
\AtBeginSubsection[]
{
  \begin{frame}<beamer>{Outline}
    \tableofcontents[currentsection,currentsubsection]
  \end{frame}
}

% Let's get started
\begin{document}

\pgfplotsset{
  compat=1.8,
  colormap={whitered}{color(0cm)=(white); color(1cm)=(orange!75!red)}
}


\begin{frame}
  \titlepage
\end{frame}


\begin{frame}[t]{Optimization}
\begin{itemize}
  \item Optimization is the process of finding the best solution to a problem from a set of possible solutions.
  \item Optimization problems come up in many applications in engineering, science, economics, biology, medicine, operations research, etc.
  \item Optimization problems can be classified in different ways, but one major classification gives us: \textbf{unconstrained} and \textbf{constrained} optimization problems.
\end{itemize}
\end{frame}


\begin{frame}[t]{A general optimization problem}
\begin{itemize}
  \item A general optimization problem can be fomulated as the following,
  \[ \begin{split}
      \min_{\mf{x} \in \mc{X}} & \,\,\, f\ct{\mf{x}} \\
      \mathrm{subject \,\, to} & \,\,\, \mf{g}\ct{\mf{x}} \leq \mf{0}, \,\, \mf{g}\ct{\mf{x}} = \bmx g_1\ct{\mf{x}} & g_2\ct{\mf{x}} & \cdots & g_p\ct{\mf{x}}\emx^\top \\
      & \,\,\, \mf{h}\ct{\mf{x}} = \mf{0}, \,\, \mf{h}\ct{\mf{x}} = \bmx h_1\ct{\mf{x}} & h_2\ct{\mf{x}} & \cdots & h_q\ct{\mf{x}}\emx^\top \\
  \end{split} \]
  where, $f\ct{\mf{x}}$ is the \textbf{objective function} and $\mf{g}\ct{\mf{x}}$ represents the set of \textbf{inquality constaints} and $\mf{h}\ct{\mf{x}}$ represents the set of \textbf{equality constraints}.

  \item In this course, we will only focus on optimization problems over $\R^n$, and mostly problems where the objective function and the constraints are differentiable.
\end{itemize}
\end{frame}


\begin{frame}[t]{A general optimization problem}
\begin{itemize}
  \item Most optimization problems of practical significance cannot be solved analytically, and we must resort to numerical iterative methods to find a solution.
  
  \item We can never solve these problems exactly through numerical means, and must content outselves with finding an approximate ``good enough'' solution.
\end{itemize}
\end{frame}


\begin{frame}[t]{Mathematical preliminaries: Sequences and Limits}
  We first review the notions of continuity and differentiability of functions of single and multiple variables, since we will be dealing with differentiable functions in optimization problems.
  \vspace{0.2cm}

  \textbf{Sequences and Limits}:
  \begin{itemize}
    \item A \text{sequence of real numbers} is a function whose domain is a set of natural numbers $1, 2, \ldots, k, \ldots$ and whose range is a set of real numbers. The sequence is denoted by $\lc x_k\rc_{k=1}^{\infty}$ or $\lc x_k \rc$.
    \vspace{0.25cm}
  
    \item A number $x^{*}$ is said to be the \textbf{limit} of the sequence $\lc x_k \rc$ if for every $\epsilon > 0$, there exists an integer $K$ such that for all $k > K$, we have $\lv x_k - x^{*} \rv < \epsilon$.
    \[ \lim_{k \to \infty} x_k = x^* \quad \mathrm{or} \quad x_k \to x^* \]

    A sequence that has a limit is called a \textbf{convergent sequence}.
  \end{itemize}
\end{frame}


\begin{frame}[t]{Sequences and Limits}
  We can extend these ideas to $\R^n$.
  \begin{itemize}
    \item A sequence in $\R^n$ is a function whose domain is a set of natural numbers $1, 2, \ldots, k, \ldots$ and whose range is $\R^n$. The sequence is denoted by $\lc \mf{x}_k\rc_{k=1}^{\infty}$ or $\lc \mf{x}_k \rc$.
    \vspace{0.25cm}
  
    \item $\mf{x}^{*}$ is said to be the \textbf{limit} of the sequence $\lc \mf{x}_k \rc$ if for every $\epsilon > 0$, there exists an integer $K$ such that for all $k > K$, we have $\lV \mf{x}_k - \mf{x}^{*} \rV < \epsilon$.
    \[ \lim_{k \to \infty} \mf{x}_k = \mf{x}^* \quad \mathrm{or} \quad \mf{x}_k \to \mf{x}^* \]

    \item The limit of a convergent sequence is unique.
  \end{itemize}
\end{frame}


\begin{frame}[t]{Continuity}
  Consider the function $f: \Omega \to \R$, where $\Omega \subseteq \R^n$. This function is continuous at the point $\mf{x}_0 \in \Omega$, if and only if, 
  \[ \lim_{\mf{x} \to \mf{x}_0} f\ct{\mf{x}} = f\ct{\mf{x}_0} \]
  
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/func_cont.pdf}
  \end{figure}
\end{frame}


\begin{frame}[t]{Differentiability}
  Differentiability is a local property of a function, like continuity. 
  
  Consider a function $f: \Omega \to \R$, where $\Omega \subseteq \R$. Let $x_0 \in \Omega$,
  \[ \frac{\delta f\ct{x_0}}{\delta x} = \frac{f\ct{x_0 + \delta x} - f\ct{x_0}}{\delta x} \]

  The function $f$ is said to be differentiable at the point $x_0 \in \Omega$, if and only if,
  \begin{itemize}
    \item $f\ct{x}$ is continuous at $x_0$.
    \item $\lim_{\delta x \to 0} \frac{\delta f\ct{x_0}}{\delta x} = \lim_{\delta x \to 0^-}\frac{\delta f\ct{x_0}}{\delta x}  = \lim_{\delta x \to 0^+} \frac{\delta f\ct{x_0}}{\delta x}$
    \item $\lim_{\delta x \to 0} \frac{f\ct{x_0}}{\delta x}$ is finite.
  \end{itemize}

  Then the derivative of the function  $f$ at the point $x_0$ is defined as,
  \[ \frac{f\ct{x_0}}{dx} = \lim_{\delta x \to 0} \frac{f\ct{x_0 + \delta x} - f\ct{x_0}}{\delta x} \]
\end{frame}


\begin{frame}[t]{Differentiability}
  Three functions $f_1, f_2, f_3$ defined over the set $\Omega = \left[ -1, 2\right] \subseteq \R$.
  
  \begin{figure}
    \centering
    \includegraphics[width=0.85\textwidth]{figs/func_diff.pdf}
  \end{figure}
\end{frame}


\begin{frame}[t]{Differentiability in $\R^n$}
  Consider the function $f: \Omega \to \R$, where $\Omega \subseteq \R^n$.
  \[ f\ct{\mf{x}} = f\ct{x_1, x_2, \ldots, x_n} \]
  $f$ maps a column vector $\mf{x} = \bmx x_1 & x_2 & \cdots & x_n \emx^\top \in \mb{R}^n$ to a real number.
  \vspace{0.25cm}

  The partial derivative of the function $f\ct{\mf{x}}$ at $\mf{x}_0$ is defined as,
  \[ \frac{\partial f\ct{\mf{x}_0}}{\partial x_i} = \lim_{\delta x \to 0} \frac{f\ct{\mf{x}_0 + \delta x \, \mf{e}_i} - f\ct{\mf{x}_0}}{\delta x} \]

  $\frac{\partial f\ct{\mf{x}}}{\partial x_i}$ is the rate of change of the function $f$ when move along the $i$-th coordinate direction at the point $\mf{x}_0$.
  \vspace{0.25cm}
  
  The function $f$ is said to be differentiable at the point $\mf{x}_0 \in \Omega$, if and only if, the partial derivatives of the function $f$ w.r.t. all $x_i$.
\end{frame}


\begin{frame}[t]{Differentiability in $\R^n$}
  The derivative of the function $f: \Omega \to \R$, where $\Omega \subseteq \R^n$ with respect to the column vector $\mf{x}$ at the point $\mf{x}_0 \in \Omega$ is defined as the following,
  \[ \nabla f\ct{\mf{x}_0} = \bmx \frac{\partial f}{\partial x_1}\ct{\mf{x}_0} & \frac{\partial f}{\partial x_2}\ct{\mf{x}_0} & \cdots & \frac{\partial f}{\partial x_n}\ct{\mf{x}_0}\emx \in \mb{R}^n \]
  Notice that $\nabla f\ct{\mf{x}_0}$ is a row vector, and it is called the \textit{gradient} of the function $f$ at the point $\mf{x}_0$.

  We follow the following convention when dealing with derivative of functions of multiple variables $f: \Omega \to \R$:
  \begin{itemize}
    \item The gradient with respect to a column vector $\mf{x}$ is a row vector $\nabla_{\mf{x}} f\ct{\mf{x}}$.
    \[ \nabla_{\mf{x}} f\ct{\mf{x}} = \bmx \frac{\partial f}{\partial x_1} & \frac{\partial f}{\partial x_2} & \cdots & \frac{\partial f}{\partial x_n} \emx \]
    \item The gradient with respect to a row vector $\mf{x}^\top$ is a column vector $\nabla_{\mf{x}^\top} f\ct{\mf{x}}$.
    \[ \nabla_{\mf{x}^\top} f\ct{\mf{x}} = \bmx \frac{\partial f}{\partial x_1} & \frac{\partial f}{\partial x_2} & \cdots & \frac{\partial f}{\partial x_n} \emx^\top \]
  \end{itemize}
\end{frame}


\begin{frame}[t]{Differentiability in $\R^n$: Jacobian of a Vector-valued function}
  Consider the function $\mf{h}: \R^q \to \R^p$, where
  \[ \mf{h}\ct{\mf{x}} = \bmx h_1\ct{\mf{x}} & h_2\ct{\mf{x}} & \cdots & h_p\ct{\mf{x}} \emx^\top \,\,\, \mf{x} \in \R^q \]

  The \textit{Jacobian} of the function $\mf{h}\ct{\mf{x}}$ with respect to $\mf{x} \in \R^q$ is defined as the following matrix,
  \[ \nabla_{\mf{x}} \mf{h}\ct{\mf{x}} \triangleq \bmxc \nabla_{\mf{x}} h_1\ct{\mf{x}} \\ \nabla_{\mf{x}} h_2\ct{\mf{x}} \\ \vdots \\ \nabla_{\mf{x}} h_q\ct{\mf{x}}\emx^\top \in \R^{p \times q}\]
\end{frame}


\begin{frame}[t]{Differentiability in $\R^n$: Hessian Matrices}
  Consider the function $f: \R^n \to \R$ and $\mf{x} \in \R^n$. 
  \vspace{0.25cm}

  The Hessian matrix $\mf{H}_f\ct{\mf{x}}$ of the function $f\ct{\mf{x}}$  is defined as the symmetric matrix $n \times n$ matrix of the second order partial derivatives of $f$ with respect to the components of $\mf{x}$, assuming all the second order partial derivatives exists.
  \vspace{0.25cm}

  The $ij^{th}$ element of the Hessian matrix of $f\ct{\mf{x}}$ is given by.
  \[ \ls \mf{H}_f\ct{\mf{x}}\rs_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}\ct{\mf{x}} = \frac{\partial}{\partial x_i}\ct{\frac{\partial f}{\partial x_j}\ct{\mf{x}}} = \frac{\partial}{\partial x_j}\ct{\frac{\partial f}{\partial x_i}\ct{\mf{x}}} \]
  % \vspace{0.25cm}

  \[ \mf{H}_f\ct{\mf{x}} \triangleq \bmxc \frac{\partial^2 f}{\partial x_1^2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
  \vdots & \ddots & \vdots \\
  \frac{\partial^2 f}{\partial x_n \partial x_1} & \cdots & \frac{\partial^2 f}{\partial x_n^2} \emx \quad \mf{H}_f\ct{\mf{x}} = \nabla_{\mf{x}^\top}\ct{\nabla_{\mf{x}} f\ct{\mf{x}}} = \nabla_{\mf{x}}\ct{\nabla_{\mf{x}^\top} f\ct{\mf{x}}} \]
\end{frame}


\begin{frame}{Gradient of a function $f: \R^n \to \R$}
  The levels set of the function $f: \R^n \to \R$ at the level $c \in \R$ is defined as,
  \[ S = \lc \mf{x} \,\, \mid \,\, \mf{x} \in \R^n. \, f\ct{\mf{x}} = c\rc \] 
  A level set is a curve for functions $f: \R^2 \to \R$, and is a surface when $f: \R^3 \to \R$.
  \vspace{0.25cm}

  The different level sets are also called the contours of the function $f$.
  \vspace{0.25cm}

  The gradient of the function $f$ at a point $\mf{x}_0$ is orthogonal to the level set of the function $f$ at the value $f\ct{\mf{x}_0}$.
  \vspace{0.25cm}

  The gradient is also the direction in $\R^n$ of maximal increase of the value of the function $f$. This is also called the direction of \textit{steepest ascent}.
\end{frame}


\begin{frame}{Taylor's Theorem}
  Many results from analysis are used in optimization problems --  one of them is the ``Taylor's'' theorem.

  The Taylor's theorem gives an polynomial approximation of a $k$ time differentiaable function $f: \R \to \R$ around at a given point $x_0$ by a $k^{th}$ order \textbf{Taylor polynomial}.

  For a smooth function (infinitely differentiable), the $k^{th}$ order Taylor polynomial is a truncation at the order $k$ of the Taylor series expansion of the function $f$ around the point $x_0$.

  \textbf{Taylor's Theoerem}: Suppose a function $f: \R \to \R$ is $k$ times differentiable at a point $x_0$, then the function $f$ can be approximated by the following polynomial with $\epsilon = x - x_0$,
  \[ f\ct{x} = f\ct{x_0} + Df\ct{x_0} \frac{\epsilon}{1!} + D^2f\ct{x_0} \frac{\epsilon^2}{2!} + \cdots + D^{k}f\ct{x_0}\frac{\epsilon^{k}}{\ct{k}!} + o\ct{\epsilon^k} \]
  where, $D^l f\ct{x_0}$ is the $l^{th}$ order derivative of the function $f$ at the point $x_0$, and $o\ct{\epsilon^k}$ is the remainder term which trends to zero faster than the function $\epsilon^k$ as $\epsilon \to 0$.
\end{frame}


\begin{frame}{Taylor's Theorem}
  Now consider a function $f: \R^n \to \R$ and $\mf{x}_0 \in \R^n$, and let's assume that $f$ is differentiable twice with respect to $\mf{x}$, and let $\bm{\epsilon} = \mf{x} - \mf{x}_0$. The polynomial approximation of $f$ is given by,
  \[ f\ct{\mf{x}} = f\ct{\mf{x}_0} + \frac{1}{1!}\mf{g}\ct{\mf{x}_0}^\top \bm{\epsilon} +  \frac{1}{2!} \bm{\epsilon}^\top \mf{H}\ct{\mf{x}_0} \bm{\epsilon} + o\ct{\Vert \bm{\epsilon} \Vert_2^2} \]
  where, $\mf{g}\ct{\mf{x}_0} = \nabla_{\mf{x}^\top}f\ct{\mf{x}_0}$ is the gradient of the function $f$ with respect to the $\mf{x}^\top$ computed at $\mf{x}_0$, and $\mf{H}\ct{\mf{x}_0}$ is the Hessian of the functon $f$ computed at $\mf{x}_0$.
\end{frame}


\begin{frame}{Local and Global Minimizers}
  We distinguih between two types of minimizers of a function $f :\R^n \to \R$:  Global and local minimizers.
  \vspace{0.25cm}

  \textbf{Global minimizer}: A point $\mf{x}^* \in \R^n$ is said to be a \textit{global minimizer} of the function $f\ct{\mf{x}}$ if and only if, $f\ct{\mf{x}^*} \leq f\ct{\mf{x}}$ for all $\mf{x} \in \R^n - \lc \mf{x}^* \rc$. 
  \vspace{0.25cm}
  
  A global minimizer is a \textit{strict global minimizer} if $f\ct{\mf{x}^*} < f\ct{\mf{x}}$ for all $\mf{x} \in \R^n - \lc \mf{x}^* \rc$.
  \vspace{0.25cm}
  
  \textbf{Local minimizer}: A point $\mf{x}^* \in \R^n$ is said to be a \textit{local minimizer} of the function $f\ct{\mf{x}}$ over the set $\Omega \subset \R^n$, if there exists $\epsilon > 0$ such that $f\ct{\mf{x}^*} \leq f\ct{\mf{x}}$ for all $\mf{x} \in \R^n - \lc \mf{x}^* \rc$ and $\Vert \mf{x} - \mf{x}^*\Vert < \epsilon$. 
  \vspace{0.25cm}

  This is a \textit{strict local minimizer} if $f\ct{\mf{x}^*} < f\ct{\mf{x}}$ for all $\mf{x} \in \R^n - \lc \mf{x}^* \rc$ and $\Vert \mf{x} - \mf{x}^*\Vert < \epsilon$.
\end{frame}


\begin{frame}{Conditions for Local Minimizers}
  Consider the twice differentiable function $f: \R^n \to \R$, and let with gradient vector $\nabla_{\mf{x}^\top}f\ct{\mf{x}}$ and Hessian matrix $\mf{H}\ct{\mf{x}}$.
  \vspace{0.1cm}

  \textbf{First order necessary condition (FONC) for local minimizers}:
  If $\mf{x}^*$ is a local minimizer of $f$, then
  \[ \nabla_{\mf{x}^\top}f\ct{\mf{x^*}} = \mf{0} \]

  \textbf{Second order necessary condition (SONC) for local minimizers}:
  If $\mf{x}^*$ is a local minimizer of $f$, then
  \[ \nabla_{\mf{x}^\top}f\ct{\mf{x^*}} = \mf{0} \quad \text{and} \quad \mf{d}^\top\mf{H}\ct{\mf{x}^*}
  \mf{d} \geq 0, \,\, \mf{d} \in \R^n \]

  \textbf{Second order sufficient condition (SONC) for local minimizers}:
  If $\mf{x}^*$ is a local minimizer of $f$, then
  \[ \nabla_{\mf{x}^\top}f\ct{\mf{x^*}} = \mf{0} \quad \text{and} \quad \mf{d}^\top\mf{H}\ct{\mf{x}^*}
  \mf{d} > 0, \,\, \mf{d} \in \R^n \]
\end{frame}


\begin{frame}{Unconstrained Optimization: Single variable case}
  Consider a function $f: \R \to \R$, and we are interested in finding the minimizer $x^*$.
  \vspace{0.25cm}

  The SOSC for this case is: $\frac{df\ct{x}}{dx} = 0$ and $\frac{d^2f\ct{x}}{dx^2} > 0$.
  \vspace{0.25cm}

  We might not be able to solve things analytically even for the single variable case, and will need to resort to iterative approaches. Such methods are called \textit{line search} methods.
  \vspace{0.25cm}

  \textbf{Iterative search methods}: We start with an initial guess $x_0$, and then update the guess using a rule,
  \[ x_{k+1} = x_k + \alpha_k h\ct{f\ct{x_k}}, \,\, k = 0, 1, 2, \ldots \]
  where, $\alpha_k$ is the step size, and $h\ct{f\ct{x_k}}$ is the search direction.
  \vspace{0.25cm}

  The iteration is continued until some stopping criteria are satisfied.
\end{frame}


\begin{frame}{Line Search Algorithm: Newton's method}
  Line search algorithms may use the value of the function $f$ at different points, the first derivative $f'$ or even the second derivative $f''$.
  \vspace{0.25cm}

  One of the most common line search methods is the Newton's methods, which uses the first and second derivatives of the function $f$ to iteratively compute a local minimizer for a function.

  At any given iteration $k$, the Newton's method uses $f\ct{x_k}$, $f'\ct{k}$, and $f''\ct{x_k}$ to fit a quadratic approximation of the function as the following,
  \[ q\ct{x} = f\ct{x_k} + f'\ct{x_k}\ct{x - x_k} + \frac{1}{2}f''\ct{x_k}\ct{x - x_k}^2 \]

  We  mininmize quadratic this approximiation $q\ct{x}$ to find the next guess $x_{k+1}$. By settinng, $q'\ct{x} = f1\ct{x_k} + f''\ct{x - x_k} = 0$, we get the next guess as,
  \[ x_{k+1} = x_k - \frac{f`\ct{x_k}'}{f''\ct{x_k}} \] 
\end{frame}


\begin{frame}{Line Search Algorithm: Secant method}
  What if we did not have access to the $f''$? We can use an approximation for $f''$ instead, which gives us the Secant method for line search.
  \vspace{0.25cm}

  \textbf{$f'$ is unknown}: We can use $f'$ to approximate $f''$ as the following,
  \[ \hat{f}''\ct{x_k} = \frac{f'\ct{x_k} - f'\ct{x_{k-1}}}{x_k - x{_k-1}} \] 
  Using this approximation in the Newton's method and simplifying the expression, we get
  \[ x_{k+1} = \frac{f'\ct{x_k}x_{k-1} - f'\ct{x_{k-1}}x_k}{f'\ct{x_k} - f'\ct{x_{k - 1}}} \]

  It left as an exercise to shown that $x_{k+1}$ is the minimizer of the quadratic approximation of the function $f$ at the point $x_k$.

\end{frame}


\begin{frame}{Line Search Algorithm: Secant method}
  Both the Newton's and Secant methods are examples of \textit{quadratic fit} methods. 
  \vspace{0.25cm}
  
  A third possible method foregoes the requirement of the first derivative $f'$ and use only the value of the function at three points to fit a quadratic approximation. 
  \vspace{0.25cm}
  
  The derivation of the iteration rule for this method is left as an exercise.
\end{frame}





% \begin{frame}[t]{Steepest descent algorithm}
% \begin{itemize}
%   \item Consider the experiment tossing a dice, and we observe the count of the dots that turn on the top face of the dice.
%   \begin{itemize}
%     \item Observed outcome is an even number. $A = \lc 2, 4, 6\rc \subset S$
%     \item Observed outcome is a positive number. $A = S \implies $ \textbf{Sure event}
%     \item Observed outcome is 0. $A = \lc\rc \implies $ \textbf{Impossible event}
%   \end{itemize}
  
%   \item For discrete sample spaces and \textbf{elementary event} is an event with just single sample point.
  
%   \item We can combine events to produce other events that might be of interest to us. Set operations can be used to perform algebra on events.
% \end{itemize}
% \end{frame}


% \begin{frame}[t]{Fundamental rules of probability}
% \begin{itemize}
%   \item Let $A$ be an event of an experiments, and $p\ct{A}$ the probability of the event $A$.
  
%   \item The assignment of probabilities satisfies the following prorperties.
%   \begin{itemize}
%     \item For any event $A$, $0 \leq P\lp A \rp \leq 1$.
%     \item $P\lp S \rp = 1$; $S$ is the sample space.
%     \item For two events $A, B$, 
%     \[ \begin{cases}
%       A \cap B = \emptyset & \implies P\lp A \cup B\rp = P\lp A\rp + P\lp B\rp\\
%       A \cap B \neq \emptyset & \implies P\lp A \cup B\rp = P\lp A\rp + P\lp B\rp - P\lp A \cap B\rp
%        \end{cases} \]
%   \end{itemize}

%   \item  The other rules for proability calculation for events of an experiment can be derived from these three axioms.
%   \begin{itemize}
%     \item $P\lp \overline{A}\rp = 1 - P\lp A \rp$
%     \item $A \subset B \, \implies P\lp A \rp \leq P\lp B\rp$
%     \item $P\lp \emptyset \rp = 0$
%     \item $P\lp A \cup B \rp = P\lp A\rp + P\lp B\rp - P\lp A \cap B\rp$
%   \end{itemize}
% \end{itemize}
% \end{frame}


% \begin{frame}[t]{Random variables}
% \begin{itemize}
%   \item A random variable is $X$ is a function that maps the sample space $S$ to the real numbers $\mb{R}$. Random variables allow us to deal with experimental outcomes and event interms of numbers instead of arbitrary symbols. Note: We will use ``r.v.'' to mean ``random variable'' from this point on.
  
%   \item Two types of random variables: Discrete random variables and Continuous random variables.
  
%   \item Discrete random variables take on values from a discrete set of numbers $\mathcal{X}$ (finite or countably infinite).
  
%   \item Continuous random variables take on values from a continuous set of numbers $\mathcal{X}$ (uncountably infinite).
  
%   \item Function that assigns probabilities to a discrete random variable $X$ is called the \textbf{proability mass function} (p.m.f.) $p\ct{X = x}$ is the proability of the random variable $X$ assuming the value $x$.
%   \[ p\ct{X = x} \geq 0, \,\,  \forall x \in \mc{X}, \quad \quad \sum_{\mc{X} \in x} p\ct{X = x} \geq 0 \]
% \end{itemize}
% \end{frame}


% \begin{frame}[t]{Random variables}
%   \vspace{-0.25cm}
%   Here are two proability mass fucntions.
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%         ycomb, % Use ycomb style for stem plot
%         width=8cm,
%         height=3.5cm,
%         ylabel={Probability},
%         ymin=0,
%         ymax=1.2,
%         xmin=0.5, % Adjust the x-axis minimum value to shift it to the left
%         xmax=4.5, % Adjust the x-axis maximum value to shift it to the left
%         xtick={1, 2, 3, 4}, % Specify the numerical x-values
%         xticklabels={$x_1$, $x_2$, $x_3$, $x_4$},
%         nodes near coords,
%         nodes near coords align={vertical},
%         axis lines=left, % Specify only left and bottom axes lines
%         title={Probability Mass Function $p\lp X \rp$}, % Add a title here
%         ]
        
%         % Define your data points and corresponding probabilities
%         \addplot +[mark=*, mark options={fill=white}] coordinates {(1, 0.2) (2, 0.3) (3, 0.1) (4, 0.4)};
%     \end{axis}
%     \end{tikzpicture}

%     \begin{tikzpicture}
%       \begin{axis}[
%         ycomb, % Use ycomb style for stem plot
%         width=8cm,
%         height=3.5cm,
%         xlabel={Random Variable $X$},
%         ylabel={Probability},
%         ymin=0,
%         ymax=1.2,
%         xmin=0.5, % Adjust the x-axis minimum value to shift it to the left
%         xmax=4.5, % Adjust the x-axis maximum value to shift it to the left
%         xtick={1, 2, 3, 4}, % Specify the numerical x-values
%         xticklabels={$x_1$},
%         nodes near coords,
%         nodes near coords align={vertical},
%         axis lines=left, % Specify only left and bottom axes lines
%         ]
        
%         % Define your data points and corresponding probabilities
%         \addplot +[mark=*, mark options={fill=white}] coordinates {(1, 1.0) };
%     \end{axis}
%     \end{tikzpicture}
%   \end{center}
% \end{frame}


% \begin{frame}{Joint and Marginal Probabilities}
%   \begin{itemize}
%     \item Consider two r.v. $X \in \mc{X}$ and $Y \in \mc{Y}$. The joint p.m.f. of these r.v. is defined as,
%     \[ p\lp X = x, Y = y\rp = p\lp \lc X = x\rc \cap \lc Y = y \rc \rp = p\lp Y = y, X = x\rp \]
%     \textbf{Meaning of joint probabilities}: $p\lp X = x, Y = y\rp$ is the probability of the r.v. $X$ takes on the value $x$ \textbf{and} the r.v. $Y$ takes on the value $y$.
    
%     \item The marginal p.m.f. of the r.v. $X$ is the probability that it takes on a value $x$. This can be computed from the joint p.m.f. as the following,
%     \[ p\lp X = x \rp = \sum_{y \in \mc{Y}} p\lp X=  x, Y = y \rp \]
%     Similary the margnal p.m.f. of r.v. $Y$ is
%     \[ p\lp Y = x \rp = \sum_{x \in \mc{X}} p\lp X =  x, Y = y \rp \]
%   \end{itemize}
% \end{frame}


% \begin{frame}{Conditional probabilities}
%   \begin{itemize}
%     \item Consider two r.v. $X \in \mc{X}$ and $Y \in \mc{Y}$, with the joint p.m.f. $p\lp X, Y\rp$.
    
%     \item The conditional p.m.f $X = x$ given $Y = y$ is defined as,
%     \[ p\lp X = x \vert Y = y \rp =  \frac{p\lp X = x, Y = y\rp}{p\lp Y = y \rp}, \,\, \mathrm{if } \,\, p\lp Y = y \rp \neq 0 \]
%     The conditional proability is not defined if $p\lp Y = y \rp = 0$.

%     \textbf{Meaning of conditional probabilities}: $p\lp X = x \vert Y = y \rp$ is the probability that r.v. $X$ taking on a value $x \in \mc{X}$, given that \textbf{we know} the r.v. $Y$ has taken on a value $y \in \mc{Y}$. 
    
%     Note that $p\lp Y = y \rp = 0$ means that $Y = y$ cannot have occured, so there is nothing to condition on (i.e., the statement ``$Y$ has taken on a value $y \in \mc{Y}$'' is meaningless).
%   \end{itemize}
% \end{frame}


% \begin{frame}{Bayes Rule}
%   Consider two discrete r.v. $X$ and $Y$. We know the following conditional probabilities,
%   \[ p\lp X \vert Y \rp =  \frac{p\lp X, Y\rp}{p\lp Y \rp} \quad \quad p\lp Y \vert X \rp =  \frac{p\lp X, Y\rp}{p\lp X \rp} \]
%   (Note: we drop writing $X=x$ and $Y=y$ for brevity).

%   Thus, we have the \textbf{Bayes rule} or \textbf{Bayes theorem},
%   \[ \begin{split}
%     p\lp X \vert Y \rp &= \frac{p\lp Y \vert X \rp p\lp X\rp}{p\lp Y \rp} = \frac{p\lp Y \vert X \rp p\lp X\rp}{\sum_{x \in \mc{X}} p\lp X = x, Y = y\rp}\\
%                        &= \frac{p\lp Y \vert X \rp p\lp X\rp}{\sum_{x \in \mc{X}} p\lp Y \vert X = x\rp p\lp X = x \rp}  \\
%   \end{split} \]
% \end{frame}


% \begin{frame}{Example of applying Bayes rule}
%   \begin{columns}
%     \begin{column}{0.7\textwidth}
%       \vspace{0.25cm}
      
%       \begin{small}
%         You have written a python program that does some clever image processing to automatically detect pulmonary embolism (PB) using a given chest x-ray image. After extensive testing with data from CMC you've estblished that your program has a sensitvity of 85\%, i.e. your program will report that a person is +ve for PB from his/her chest x-ray image 85\% of the time when the person is indeed +ve for PB. And it has a specificity of 95\%, i.e. your program will report that a person is -ve for PB from his/her chest x-ray image 95\% of the time when the person is indeed -ve for PB. 
%         \vspace{0.1cm}
        
%         When I run your program on my most recent chest x-ray, your program reported that I am +ve for PB! Oh my god! Do I have PB? What is the probability that I have PB?
%       \end{small}
%     \end{column}
%     \begin{column}{0.275\textwidth}
%       \begin{figure}
%         \centering
%         \includegraphics[width=0.8\textwidth]{figs/scared.png}
%       \end{figure}
%     \end{column}
%   \end{columns}
% \end{frame}


% \begin{frame}{Independence}
%   We say two r.v. $X$ and $Y$ are unconditionally independent or marginally independent, denoted by $X \perp Y$, if
%   \[ X \perp Y \quad \iff \quad p\ct{X, Y} = p\ct{X}p\ct{Y} \]
    
%   \textcolor{blue}{What does this mean?}
%   \begin{itemize}
%     \item The two r.v. do not carry any information about the other. \textcolor{gray}{Remember the $\perp$ symbol when talking about vectors. $\mf{x} \perp \mf{y} \implies$ $\mf{x}$ is perpendicular to $\mf{y}$. Informally, $\mf{x}$ does not carry any information about $y$ and \textit{vice versa}. The same idea applies here r.v. $X$ and $Y$. $X \perp Y \implies$ that r.v. $X$ contains no information about $Y$ and \textit{vice versa}.}
%     \item The condition probability is the marginal probability, i.e. $p\ct{X \vert Y} = p\ct{X}$ and $p\ct{Y \vert X} = p\ct{Y}$.
%     \item The p.m.f. of $X$ for any given values of $Y$ has the same shape as $p\ct{X}$, and similarly the p.m.f. of $Y$ for any given value of $X$ has the same shape as $p\ct{Y}$.
%     \[ p\ct{X, Y=y} \propto p\ct{X} \quad \quad p\ct{X=x, Y} \propto p\ct{Y} \]
%   \end{itemize}
% \end{frame}


% \begin{frame}{Conditional Independence}
%   We say two r.v. $X$ and $Y$ are conditionally independent given a r.v. $Z$, denoted by $X \perp Y \vert Z$, if
%   \[ X \perp Y \vert Z \quad \iff \quad p\ct{X, Y \vert Z} = p\ct{X \vert Z}p\ct{Y \vert Z} \]
    
%   \textcolor{blue}{What does this mean?} $X$ carries not information about $Y$, and \textit{vice versa}, given that we know $Z$ took on some value $z$.
%   \vspace{0.2cm}

%   \textcolor{blue}{Theorem}:
%   $X \perp Y \vert Z$ if and only if, there exist  functions $g$ and $h$ such that,
%   \[ p\ct{X, Y \vert Z} = g\ct{X, Z} h\ct{Y, Z} \]
%   for all $X, Y$ such that $p\ct{Z} > 0$.
% \end{frame}


% \begin{frame}{Continuous Random Variables}
%   \begin{itemize}
%     \item Let $X$ be a continuous r.v. such that $X \in \mc{X} \subseteq \mb{R}$.
%     \item We can meaningfully define probabilities for continuous r.v. only for intervals of the real line. For example, we can define the probability that $X$ takes on a value in the interval $\ls a, b \rs \subset \mc{X}$.
%     \item For a continuous r.v. $X$, we define a probability density function (p.d.f.) $f\ct{x}$ such that,
%     \[ p\ct{a \leq X \leq b} = \int_{a}^{b} f\ct{X}dX \]
%     Another useful function is the cummulative distribution function (c.d.f.) $F\ct{X}$, defined as,
%     \[ p\ct{X \leq a} = F\ct{X} = \int_{-\infty}^{a} f\ct{X}dX \]
%     \item For a small interval $\ls x, x + dx \rs$, the probability that $X$ takes on a value in this interval is $f\ct{X}dx$ $\longrightarrow f\ct{X} = \frac{p\ct{x, x+dx}}{dx}$.
%   \end{itemize}
% \end{frame}


% \begin{frame}{Expected values of a random varaible}
%   \textbf{Expected value of a r.v} is the average value of the r.v. over all possible outcomes. For a discrete r.v. $X$ with p.m.f. $p\ct{X}$, the expected value is,
%   \[ \mb{E}\dt{X} = \sum_{x \in \mc{X}} x \cdot p\ct{X = x} \]
%   For a continuous r.v. $X$ with p.d.f. $f\ct{X}$, the expected value is,
%   \[ \mb{E}\dt{X} = \int_{\mc{X}} x \cdot f\ct{X = x} dX \quad \mathrm{or} \mb{E}\dt{X} = \sum_{x \in \mc{X}} x \cdot p\ct{X = x} \]
% \end{frame}


% \begin{frame}{Expected values of a random varaible}
%   \textbf{Variance a r.v} is a measure of the spread of a r.v. about its mean.
%   \[ \mathrm{var}\dt{X} = \mb{E}\dt{\ct{X - E\dt{X}}^2} = \mb{E}\dt{X^2} - \mb{E}\dt{X}^2 \]
  
%   The square root of $\mathrm{var}\dt{X}$ is called the \textbf{standard deviation} of $X$.
%   \[ \mathrm{std}\dt{X} = \sqrt{\mathrm{var}\dt{X} } \]

%   We can compute the expected value of any function $g\ct{\bullet}$ of a r.v. $X$ as follows,
%   \[ \mb{E}\dt{g\ct{X}} = \int_{\mc{X}} g\ct{X} \cdot f\ct{X} dX \]
% \end{frame}


% \begin{frame}{Covariance and Correlation between two r.v. $X$ and $Y$}
%   Consider two r.v. $X$ and $Y$ with joint p.d.f. $f\ct{X, Y}$. The covariance between $X$ and $Y$ measures the (linear) relationship between the two r.v. This is defined as the following,
%   \[ \mathrm{cov}\dt{X, Y} = \mb{E}\dt{\ct{X - \mb{E}\dt{X}}\ct{Y - \mb{E}\dt{Y}}} \]
%   \vspace{0.05cm}
  
%   $\mathrm{cov}\dt{X, Y}$ can take on any value between $-\infty$ and $\infty$. 
%   \vspace{0.4cm}
  
%   When $\mathrm{cov}\dt{X, Y}$ is normalized by the standard deviations of $X$ and $Y$, we get the correlation between $X$ and $Y$.
%   \[ \mathrm{corr}\dt{X, y} = \frac{\mathrm{cov}\dt{X, Y}}{\sqrt{\mathrm{var}\dt{X}} \sqrt{\mathrm{var}\dt{Y}}} \] 
% \end{frame}


% \begin{frame}{Some discrete r.v. and their p.m.f.}
%   \textbf{Bernoulli distribution} Used to model a single coin toss. The r.v. $X \in \lc 0, 1\rc$ takes on the value 1 if the coin lands heads, and 0 if the coin lands tails. The p.m.f. is,
%   \[ p\ct{X = x; \theta} = \theta^X \cdot \ct{1 - \theta}^{\ct{1 - X}} \]
%   where, $p$ is the probability of the coin turning up heads.
%   \vspace{0.25cm}
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%         ycomb, % Use ycomb style for stem plot
%         width=4cm,
%         height=3.5cm,
%         ylabel={\small Probability},
%         ymin=0,
%         ymax=1.2,
%         xmin=-0.25, % Adjust the x-axis minimum value to shift it to the left
%         xmax=1.25, % Adjust the x-axis maximum value to shift it to the left
%         xtick={0, 1}, % Specify the numerical x-values
%         % xticklabels={$0$, $x_2$, $x_3$, $x_4$},
%         nodes near coords,
%         nodes near coords align={vertical},
%         axis lines=left, % Specify only left and bottom axes lines
%         title={\small Bernoulli p.m.f. $p\lp X \rp$}, % Add a title here
%         ]
        
%         % Define your data points and corresponding probabilities
%         \addplot +[mark=*, mark options={fill=white}] coordinates {(0, 0.3) (1, 0.7)};
%       \end{axis}
%     \end{tikzpicture}
%   \end{center}
% \end{frame}


% \begin{frame}{Some discrete r.v. and their p.m.f.}
%   \textbf{Bionomial distribution} Used to model the result of experiment with $n$ independent coin tosses. The r.v. $X \in \lc 0, 1, \ldots, n\rc$ takes on the value $k$ if there are $k$ heads in $n$ tosses. The p.m.f. is,
%   \[ p\ct{X = k; \theta, n} =  \frac{n!}{k! \ct{n - k}!} \cdot \theta^k \cdot \ct{1 - \theta}^{\ct{n - k}} \]
% \end{frame}


% \begin{frame}{Some discrete r.v. and their p.m.f.}
%   \textbf{Poisson distribution} Used to model the number of events that occur in a fixed interval of time. The r.v. $X \in \lc 0, 1, \ldots\rc$ takes on the value $k$ if there are $k$ events in the interval. The p.m.f. is,
%   \[ p\ct{X = k; \lambda} =  \frac{\lambda^k}{k!} e^{-\lambda} \]
%   where, $\lambda$ is the average number of events in the interval.
% \end{frame}


% \begin{frame}{Some discrete r.v. and their p.m.f.}
%   \textbf{Uniform distribution} Used to model the outcome of an experiment where all outcomes are equally likely. The r.v. $X \in \lc a, b\rc$ takes on the value $x$ with equal probability. The p.m.f. is,
%   \[ \mathrm{Unif}\ct{X = x; a, b} =  \frac{1}{b - a} \mb{I}\ct{a \leq x \leq b} \]
%   where, $\mb{I}\ct{A}$ is the indicator function, defined as the following
%   \[ \mb{I}\ct{A} = \begin{cases}
%     1 & \mathrm{if} \,\, A \,\, \mathrm{is} \,\, \mathrm{true}\\
%     0 & \mathrm{if} \,\, A \,\, \mathrm{is} \,\, \mathrm{false} 
%   \end{cases} \]
% \end{frame}


% \begin{frame}{Gaussian (Normal) distribution}
%   \textbf{Gaussian Distribution} is the most commonly used statistical distribution, wose p.m.f. is defined as,
%   \[ \mc{N}\ct{X=x; \mu, \sigma^2 } = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{\ct{x - \mu}^2}{2\sigma^2}} \]
%   where, $\mu$ is the mean of the distribution and $\sigma^2$ is the variance.
%   \vspace{0.1cm}

%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%         width=10cm,
%         height=5cm,
%         xlabel={$x$},
%         ylabel={$\mc{N}(X; \mu, \sigma^2)$},
%         legend pos=north west,
%         axis lines=left, % Specify only left and bottom axes lines
%         ]
        
%         % Define parameters for the first Gaussian distribution
%         \newcommand\muA{0}
%         \newcommand\sigmaA{1}
        
%         % Plot the first Gaussian distribution
%         \addplot[domain=-5:5, samples=100, blue] {1/(\sigmaA*sqrt(2*pi))*exp(-(x-\muA)^2/(2*\sigmaA^2))};
        
%         % Define parameters for the second Gaussian distribution
%         \newcommand\muB{2}
%         \newcommand\sigmaB{0.5}
        
%         % Plot the second Gaussian distribution
%         \addplot[domain=-5:5, samples=200, red] {1/(\sigmaB*sqrt(2*pi))*exp(-(x-\muB)^2/(2*\sigmaB^2))};
%       \end{axis}
%     \end{tikzpicture}
%   \end{center}
% \end{frame}


% \begin{frame}{Gaussian (Normal) distribution}
%   \begin{itemize}
%     \item It is commonly observed in nature that many quantities follow a Gaussian distribution.
%     \item Central limit theorem shows that the sum of a large number of independent random variables is approximately Gaussian.
%     \item Its parameters $\mu$ and $\sigma^2$ have easy interpretations.
%     \item Gaussian distribution is the maximum entropy distribution for a given mean and variance; i.e. it makes the least assumption about the parameter being modelled once we choose the mean and variance.
%   \end{itemize}
% \end{frame}


% \begin{frame}{Multivariate Gaussian (Normal) distribution}
%   The multivariate Gaussian distribution is commonly use for modelling the joint p.m.f. of multiple r.v.s $X_1, X_2, X_3, \ldots X_n$. Let's represent the r.v.s as a vector $\mf{x} = \bmx X_1 & X_2 & X_3 & \ldots & X_n \emx^\top$. The p.d.f. of the multivariate Gaussian distribution is defined as,
%   \[ \mc{N}\ct{\mf{x}; \bm{\mu}, \bm{\Sigma}} = \frac{1}{\ct{2\pi}^{n/2} \vert \bm{\Sigma} \vert^{1/2}} \exp\ct{-\frac{1}{2}\ct{\mf{x} - \bm{\mu}}^\top\bm{\Sigma}^{-1}\ct{\mf{x} - \bm{\mu}}} \]

%   where, $\bm{\mu} = \mb{E}\dt{\mf{x}} = \bmx \mb{E}\dt{X_1} & \mb{E}\dt{X_2} & \cdots \mb{E}\dt{X_n} \emx^\top$ is the mean of the distribution, and $\bm{\Sigma} = \mathrm{cov}\dt{\mf{x}}$ is the covariance matrix of the distribution.
%   \[ \begin{split}
%     \bm{\Sigma} = &\mathrm{cov}\dt{\mf{x}} = \mb{E}\dt{\mf{x}\mf{x}^\top} \\
%     = & \bmxc 
%     \mathrm{cov}\dt{X_1, X_1} & \mathrm{cov}\dt{X_1, X_2} & \cdots & \mathrm{cov}\dt{X_1, X_n} \\
%     \mathrm{cov}\dt{X_2, X_1} & \mathrm{cov}\dt{X_2, X_2} & \cdots & \mathrm{cov}\dt{X_2, X_n} \\
%     \vdots & \vdots & \ddots & \vdots \\ 
%     \mathrm{cov}\dt{X_n, X_1} & \mathrm{cov}\dt{X_n, X_2} & \cdots & \mathrm{cov}\dt{X_n, X_n}
%     \emx
%   \end{split} \]
% \end{frame}

% \begin{frame}{Multivariate Gaussian Distribution}
%   \begin{columns}
%     \begin{column}{0.7\textwidth}
%       \vspace{0.25cm}
%       \begin{tikzpicture}[
%         declare function={mu1=1;},
%         declare function={mu2=2;},
%         declare function={sigma1=0.5;},
%         declare function={sigma2=1.0;},
%         declare function={normal(\m,\s)=1/(2*\s*sqrt(pi))*exp(-(x-\m)^2/(2*\s^2));},
%         declare function={bivar(\ma,\sa,\mb,\sb)=
%         1/(2*pi*\sa*\sb) * exp(-((x-\ma)^2/\sa^2 + (y-\mb)^2/\sb^2))/2;}]
%         \begin{axis}[
%           colormap name=whitered,
%           width=9.5cm,
%           view={50}{45},
%           enlargelimits=false,
%           grid=major,
%           domain=-1:4,
%           y domain=-1:4,
%           samples=52,
%           xlabel=$X_1$,
%           ylabel=$X_2$,
%           % axis on top, % Ensure grid lines are drawn on top
%           ]
%           \addplot3 [surf] {bivar(mu1,sigma1,mu2,sigma2)};
%           \draw [black!90] (axis cs:-1,0,0) -- (axis cs:4,0,0);
%           \draw [black!90] (axis cs:0,-1,0) -- (axis cs:0,4,0);      
%         \end{axis}
%       \end{tikzpicture}
%     \end{column}
%     \begin{column}{0.275\textwidth}
%       \[ \bm{\mu} = \bmx 1 & 2 \emx \]
%       \[ \bm{\Sigma} = \bmx 0.5 & 0 \\ 0 & 1.0 \emx \]
%     \end{column}
%   \end{columns}
% \end{frame}

\end{document}